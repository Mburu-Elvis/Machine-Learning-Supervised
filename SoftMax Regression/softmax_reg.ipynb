{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "Also called ***Multinominal Logistic Regression***\n",
    "\n",
    "It's a generalization of logistic regression to the case where we want to handle multiple classes\n",
    "\n",
    "Softmax regression allows us to handle y<sup>i</sup> = {1, ..., K} where <bold>K</bold> is the number of classes\n",
    "\n",
    "In Softmax regression we are interested in multiclass classification, thus label ***y*** can take on ***K*** diffferent values, as oppossed to two in ***Logistic Regression***\n",
    "The softmax function is used to compute the probabilities of an instances belonging to each class, the class with the highest probabilities is chosen as the predicted class\n",
    "\n",
    "The regression  model first computes as score for each k, then the probability of each class by applying the ***softmax function*** to scores.\n",
    "\n",
    "### Score Calculation/ Linear combination\n",
    "\n",
    "For a given instance x, the Softmax Regression model calculates a score s<sub>k</sub> for each class K\n",
    "\n",
    "The score is obtained by performing a linear combination of the input features ***x***\n",
    "with the associated weights ***w<sub>k</sub>*** for class k, and adding a bias term ***b<sub>k</sub>***:-\n",
    "\n",
    "> s<sub>k</sub> = x.w<sub>k</sub> + b<sub>k</sub>\n",
    "\n",
    "where:-\n",
    "- ***x*** is the input vector\n",
    "- ***w<sub>k</sub> is the weight vector ***k***\n",
    "- ***b<sub>k</sub> is the bias term for class ***k***\n",
    "\n",
    "### Softmax Function(Normalization):\n",
    "\n",
    "The softmax function is applied to the scores, after all the s<sub>k</sub> are computed for all classes, to obtain normalized probabilities for each class\n",
    "\n",
    "The softmax function takes the exponentials of each scores and then normalizes them by dividing each exponential by the sum of all exponentials\n",
    "\n",
    "> P(y = k /x) = (e<sup>s<sub>k</sub></sub>x) / (sum<sup>k</sup><sub>j = 1</sub> e<sup>s<sub>j</sub>x</sup>) = (exp(s<sub>k</sub>x)/∑<sub>j = 1</sub> exp s<sub>j</sub> x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K is total number of classes\n",
    "- the numerator calculates the unnormalized probability of the instance belonging to class ***K*** based on its score\n",
    "- the denominator sums the unnormalized probailities of all classes\n",
    "> This ensures the probabilities add up to 1 as it is with probability\n",
    "\n",
    "The of softmax regression predicts the class with the highest probability\n",
    "\n",
    "> prediction (x) = argmax<sub>k</sub> P(y = k/x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> P(y=k∣x)= e<sup>s<sub>k</sub>x</sup> / ∑<sub><sup>k​</sup>j=1</sub>e<sup>s<sub>j</sub>​x</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> P(y=i∣x) = e<sup>x⋅w<sub>i</sub>​+b<sub>i</sub></sup> / ∑<sup>k​</sup><sub>j=1</sub>e<sup>x⋅w<sub>j</sub>​+b<sub>j</sub>​​</sup>\n",
    "\n",
    "Where:\n",
    "\n",
    "- ***s*** is the input feature vector\n",
    "- ***w<sub>i</sub>*** is the weight vector associated with class ***i***\n",
    "- ***b<sub>i</sub>*** is the bias term associated with class ***i***\n",
    "- ***k*** is the nmber of classes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
