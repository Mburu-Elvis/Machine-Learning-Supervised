{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive bayes**\n",
    "\n",
    "Naive Bayes is a probabilistic machine learning algorithm that is commonly used for classification and text mining tasks. It's based on Bayes' theorem, a fundamental concept in probability theory, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event.<br>\n",
    "Naïve Bayes Classifier uses the Bayes’ theorem to predict membership probabilities for each class such as the probability that given record or data point belongs to a particular class. The class with the highest probability is considered as the most likely class. This is also known as the Maximum A Posteriori (MAP).<br>\n",
    "The MAP for a hypothesis with 2 events A and B is\n",
    "\n",
    "MAP (A)\n",
    "\n",
    "= max (P (A | B))\n",
    "\n",
    "= max (P (B | A) * P (A))/P (B)\n",
    "\n",
    "= max (P (B | A) * P (A))\n",
    "\n",
    "Here, P (B) is evidence probability. It is used to normalize the result. It remains the same, So, removing it would not affect the result.\n",
    "\n",
    "Naïve Bayes Classifier assumes that all the features are unrelated to each other. Presence or absence of a feature does not influence the presence or absence of any other feature.\n",
    "\n",
    "**Types of Naive Bayes Algorithm**\n",
    "\n",
    "    1.Gaussian Naïve Bayes-When we have continuous attribute values, we made an assumption that the values associated with each class are distributed according to Gaussian or Normal distribution. For example, suppose the training data contains a continuous attribute x. \n",
    "\n",
    "    2.Multinomial Naïve Bayes-With a Multinomial Naïve Bayes model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial (p1, . . . ,pn) where pi is the probability that event i occurs. Multinomial Naïve Bayes algorithm is preferred to use on data that is multinomially distributed. It is one of the standard algorithms which is used in text categorization classification.\n",
    "\n",
    "    3.Bernoulli Naïve Bayes-In the multivariate Bernoulli event model, features are independent boolean variables (binary variables) describing inputs. Just like the multinomial model, this model is also popular for document classification tasks where binary term occurrence features are used rather than term frequencies.\n",
    "\n",
    "**Applications of Naive bayes**\n",
    "\n",
    "Naïve Bayes is one of the most straightforward and fast classification algorithm. It is very well suited for large volume of data. It is successfully used in various applications such as :\n",
    "\n",
    "    .Spam filtering-spam filtering protocols use instance-based or memory-based learning methods to identify and classify incoming spam emails based on their resemblance to stored training examples of spam emails. \n",
    "\n",
    "    .Text classification-The naive Bayes algorithms are known to perform best on text classification problems. The algorithm is mainly used when there is a problem statement related to the text and its classification. Several naive Bayes algorithms are tried and tuned according to the problem statement and used for a better accurate model. For Example: classifying the tags from the text etc.\n",
    "\n",
    "    .Sentiment analysis-Algorithms like Bernoulli naive are used most for these sentiment analysis problems. This algorithm is known to outperform on binary classification problems and is hence used most for such cases.\n",
    "    \n",
    "    .Recommender systems-\n",
    "        There are a total of two recommendation systems, content-based and collaborative filtering. The naive Bayes with collaborative filtering-based models is known for their best accuracy on recommendation problems. The naive Bayes algorithms help achieve better accuracies for recommending features to the users based on their interests and related to other users’ interests.\n",
    "\n",
    "**Advantages of  Naive bayes**\n",
    "\n",
    "**.**Faster algorithms-The Naive Bayes algorithm is a parametric algorithm that tries to assume certain things while training and using the knowledge for prediction. Hence it takes significantly less time for prophecy and is a faster algorithm.\n",
    "\n",
    "**.**Less training data-The naive Bayes algorithm assumes the independent features to be independent of each other, and if it exists, then the naive Bayes needs less data for training and performs better.\n",
    "\n",
    "**.**Perfomance-The Naive Bayes algorithm performs faster and more accurately on fewer data, and the algorithm’s results on categorical text data are overpowered, which can not be compared to other algorithms.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "**.**Independent features-In a real-time dataset, it is almost impossible to get independent features that are entirely independent of each other, there are nearly two to three features that are correlated with each other, and hence the assumption is not fully satisfied all the time.\n",
    "\n",
    "**.**Zero frequency error-According to this error, if a category is not present in the training data and in the test data, then the Naive Bayes algorithm will assign it zero probability, and the error is called Zero Frequency error in Naive Bayes.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
